{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9140298,"sourceType":"datasetVersion","datasetId":5514306}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import statements\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport urllib.request # to import images from dataset\nimport sys\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:47.201887Z","iopub.execute_input":"2025-03-01T13:31:47.202092Z","iopub.status.idle":"2025-03-01T13:31:48.081106Z","shell.execute_reply.started":"2025-03-01T13:31:47.202072Z","shell.execute_reply":"2025-03-01T13:31:48.080181Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Import Neural Network and PyTorch Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as f\nimport torch.optim as optim\n\nimport torchvision\nfrom torchvision.transforms import v2\nfrom torchvision import models\nfrom torchvision.models import resnet50, ResNet50_Weights\nimport torch.optim.lr_scheduler as lr_scheduler\n\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:48.083401Z","iopub.execute_input":"2025-03-01T13:31:48.083912Z","iopub.status.idle":"2025-03-01T13:31:52.857005Z","shell.execute_reply.started":"2025-03-01T13:31:48.083881Z","shell.execute_reply":"2025-03-01T13:31:52.856274Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"CUDA is available. Using GPU.\")\nelse:\n    device = torch.device('cpu')\n    print(\"CUDA is not available. Using CPU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:52.857938Z","iopub.execute_input":"2025-03-01T13:31:52.858261Z","iopub.status.idle":"2025-03-01T13:31:52.894816Z","shell.execute_reply.started":"2025-03-01T13:31:52.858238Z","shell.execute_reply":"2025-03-01T13:31:52.893911Z"}},"outputs":[{"name":"stdout","text":"CUDA is available. Using GPU.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### **Importing data**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fakeddit-dataset/multimodal_only_samples/multimodal_train.tsv', sep='\\t')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:52.895843Z","iopub.execute_input":"2025-03-01T13:31:52.896151Z","iopub.status.idle":"2025-03-01T13:31:57.895998Z","shell.execute_reply.started":"2025-03-01T13:31:52.896131Z","shell.execute_reply":"2025-03-01T13:31:57.895161Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"1. Remove the columns for \"2-way\" and \"3-way\" classification\n2. Remove the column \"title\" since we already have the \"clear_title\" column","metadata":{}},{"cell_type":"code","source":"df.drop(['6_way_label', '3_way_label', 'title'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:57.897005Z","iopub.execute_input":"2025-03-01T13:31:57.897231Z","iopub.status.idle":"2025-03-01T13:31:58.006122Z","shell.execute_reply.started":"2025-03-01T13:31:57.897214Z","shell.execute_reply":"2025-03-01T13:31:58.005399Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.007203Z","iopub.execute_input":"2025-03-01T13:31:58.007537Z","iopub.status.idle":"2025-03-01T13:31:58.025439Z","shell.execute_reply.started":"2025-03-01T13:31:58.007507Z","shell.execute_reply":"2025-03-01T13:31:58.024565Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           author                                        clean_title  \\\n0     Alexithymia  my walgreens offbrand mucinex was engraved wit...   \n1        VIDCAs17                this concerned sink with a tiny hat   \n2  prometheus1123      hackers leak emails from uae ambassador to us   \n3             NaN                           puppy taking in the view   \n4       3rikR3ith               i found a face in my sheet music too   \n\n    created_utc         domain  hasImage      id  \\\n0  1.551641e+09    i.imgur.com      True  awxhir   \n1  1.534727e+09      i.redd.it      True  98pbid   \n2  1.496511e+09  aljazeera.com      True  6f2cy5   \n3  1.471341e+09    i.imgur.com      True  4xypkv   \n4  1.525318e+09      i.redd.it      True  8gnet9   \n\n                                           image_url linked_submission_id  \\\n0  https://external-preview.redd.it/WylDbZrnbvZdB...                  NaN   \n1  https://preview.redd.it/wsfx0gp0f5h11.jpg?widt...                  NaN   \n2  https://external-preview.redd.it/6fNhdbc6K1vFA...                  NaN   \n3  https://external-preview.redd.it/HLtVNhTR6wtYt...                  NaN   \n4  https://preview.redd.it/ri7ut2wn8kv01.jpg?widt...                  NaN   \n\n   num_comments  score          subreddit  upvote_ratio  2_way_label  \n0           2.0     12  mildlyinteresting          0.84            1  \n1           2.0    119         pareidolia          0.99            0  \n2           1.0     44        neutralnews          0.92            1  \n3          26.0    250   photoshopbattles          0.95            1  \n4           2.0     13         pareidolia          0.84            0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>clean_title</th>\n      <th>created_utc</th>\n      <th>domain</th>\n      <th>hasImage</th>\n      <th>id</th>\n      <th>image_url</th>\n      <th>linked_submission_id</th>\n      <th>num_comments</th>\n      <th>score</th>\n      <th>subreddit</th>\n      <th>upvote_ratio</th>\n      <th>2_way_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alexithymia</td>\n      <td>my walgreens offbrand mucinex was engraved wit...</td>\n      <td>1.551641e+09</td>\n      <td>i.imgur.com</td>\n      <td>True</td>\n      <td>awxhir</td>\n      <td>https://external-preview.redd.it/WylDbZrnbvZdB...</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>12</td>\n      <td>mildlyinteresting</td>\n      <td>0.84</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>VIDCAs17</td>\n      <td>this concerned sink with a tiny hat</td>\n      <td>1.534727e+09</td>\n      <td>i.redd.it</td>\n      <td>True</td>\n      <td>98pbid</td>\n      <td>https://preview.redd.it/wsfx0gp0f5h11.jpg?widt...</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>119</td>\n      <td>pareidolia</td>\n      <td>0.99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>prometheus1123</td>\n      <td>hackers leak emails from uae ambassador to us</td>\n      <td>1.496511e+09</td>\n      <td>aljazeera.com</td>\n      <td>True</td>\n      <td>6f2cy5</td>\n      <td>https://external-preview.redd.it/6fNhdbc6K1vFA...</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>44</td>\n      <td>neutralnews</td>\n      <td>0.92</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>puppy taking in the view</td>\n      <td>1.471341e+09</td>\n      <td>i.imgur.com</td>\n      <td>True</td>\n      <td>4xypkv</td>\n      <td>https://external-preview.redd.it/HLtVNhTR6wtYt...</td>\n      <td>NaN</td>\n      <td>26.0</td>\n      <td>250</td>\n      <td>photoshopbattles</td>\n      <td>0.95</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3rikR3ith</td>\n      <td>i found a face in my sheet music too</td>\n      <td>1.525318e+09</td>\n      <td>i.redd.it</td>\n      <td>True</td>\n      <td>8gnet9</td>\n      <td>https://preview.redd.it/ri7ut2wn8kv01.jpg?widt...</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>13</td>\n      <td>pareidolia</td>\n      <td>0.84</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.026397Z","iopub.execute_input":"2025-03-01T13:31:58.026645Z","iopub.status.idle":"2025-03-01T13:31:58.031943Z","shell.execute_reply.started":"2025-03-01T13:31:58.026626Z","shell.execute_reply":"2025-03-01T13:31:58.031034Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(564000, 13)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"- Apply `train_test_split()` to minimize dataset with over 564.000 samples.\n\n- Split the dataset and only use 5% data to train (because of limited resources).\n\n- Stratify function is applied in order to maintain the same class sample distribution (percentage) from original Fakeddit source dataset.","metadata":{}},{"cell_type":"code","source":"df, _ = train_test_split(\n    df,\n    test_size=0.97,\n    shuffle=True,\n    stratify=df[\"2_way_label\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.034814Z","iopub.execute_input":"2025-03-01T13:31:58.035062Z","iopub.status.idle":"2025-03-01T13:31:58.525367Z","shell.execute_reply.started":"2025-03-01T13:31:58.035043Z","shell.execute_reply":"2025-03-01T13:31:58.524401Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.526724Z","iopub.execute_input":"2025-03-01T13:31:58.527302Z","iopub.status.idle":"2025-03-01T13:31:58.532565Z","shell.execute_reply.started":"2025-03-01T13:31:58.527267Z","shell.execute_reply":"2025-03-01T13:31:58.531721Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(16920, 13)"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### **Data visualization**","metadata":{}},{"cell_type":"code","source":"df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.533567Z","iopub.execute_input":"2025-03-01T13:31:58.533879Z","iopub.status.idle":"2025-03-01T13:31:58.542488Z","shell.execute_reply.started":"2025-03-01T13:31:58.533833Z","shell.execute_reply":"2025-03-01T13:31:58.541600Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Check for Nan values\nprint(\"clean_title:\", df[\"clean_title\"].isnull().sum())\nprint(\"has_image:\", df[\"hasImage\"].isnull().sum())\nprint(\"id:\", df[\"id\"].isnull().sum())\n\n# Check how many rows with the column \"hasImage\" is false\nprint(\"\\n\", df[\"hasImage\"].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.543488Z","iopub.execute_input":"2025-03-01T13:31:58.543783Z","iopub.status.idle":"2025-03-01T13:31:58.565415Z","shell.execute_reply.started":"2025-03-01T13:31:58.543756Z","shell.execute_reply":"2025-03-01T13:31:58.564676Z"}},"outputs":[{"name":"stdout","text":"clean_title: 0\nhas_image: 0\nid: 0\n\n hasImage\nTrue    16920\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"**Plot the 6- Way Class Distribution of dataset:**\n- 0: TRUE\n- 1: SATIRE\n- 2: FALSE CONNECTION\n- 3: IMPOSTER CONTENT\n- 4: MANIPULATED CONTENT\n- 5: MISLEADING CONTENT","metadata":{}},{"cell_type":"code","source":"df[\"2_way_label\"].plot(kind=\"hist\", bins=15, title=\"2_way_label\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.566423Z","iopub.execute_input":"2025-03-01T13:31:58.566671Z","iopub.status.idle":"2025-03-01T13:31:58.849732Z","shell.execute_reply.started":"2025-03-01T13:31:58.566652Z","shell.execute_reply":"2025-03-01T13:31:58.848896Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<Axes: title={'center': '2_way_label'}, ylabel='Frequency'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"> Based on the class distribution graph above, the classes are not represented equally in the dataset.","metadata":{}},{"cell_type":"markdown","source":"### **Import images from URL into working directory**","metadata":{}},{"cell_type":"markdown","source":"- Used the `request` module from `urllib` library to handle the downloading of images from URLs.\n\n- Any NaN values in the DataFrame are replaced with empty strings using `replace()` and `fillna()` methods. This ensures that there are no missing values when working with the data.\n\n- Used `urlopen()` to download the image from the URL and write the content to a file. The file is saved with the id of the row and the extension .jpg, to enable easy future access\n\n- Some image URLs do not exist anymore/ have been taken down, the corresponding rows are dropped from the DataFrame.","metadata":{}},{"cell_type":"code","source":"# from urllib import request\n\n# # Replace NaN values with empty strings\n# df = df.replace(np.nan, '', regex=True)\n# df.fillna('', inplace=True)\n\n# # Make a directory to download images into\n# if not os.path.exists(\"/kaggle/working/images\"):\n#   os.makedirs(\"/kaggle/working/images\")\n\n# for index, row in df.iterrows():\n#   if row[\"hasImage\"] == True and row[\"image_url\"] != \"\" and row[\"image_url\"] != \"nan\":\n#     image_url = row[\"image_url\"]\n#     path = \"/kaggle/working/images/\" + row[\"id\"] + \".jpg\"\n\n#     try:\n#       f = open(path, 'wb')\n#       f.write(request.urlopen(image_url).read())\n#       f.close()\n\n#     except:\n#         # To account for now invalid image urls\n#         df.drop(index=index, axis=0, inplace=True)\n#         pass\n\n# print(\"Downloaded all images.\")\n# df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.850908Z","iopub.execute_input":"2025-03-01T13:31:58.851227Z","iopub.status.idle":"2025-03-01T13:31:58.855702Z","shell.execute_reply.started":"2025-03-01T13:31:58.851199Z","shell.execute_reply":"2025-03-01T13:31:58.854756Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom urllib import request\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Ensure df is defined (for testing, replace with actual DataFrame loading)\n# df = pd.read_csv(\"your_file.csv\")\n\n# Replace NaN values with empty strings\ndf = df.replace(np.nan, '', regex=True)\ndf.fillna('', inplace=True)\n\n# Create the directory to store images if it doesn't exist\nimage_dir = \"/kaggle/working/images\"\nos.makedirs(image_dir, exist_ok=True)\n\n# Function to download a single image\ndef download_image(row):\n    image_url = row[\"image_url\"]\n    image_path = os.path.join(image_dir, f\"{row['id']}.jpg\")\n\n    # Skip if image already exists\n    if os.path.exists(image_path):\n        return True  \n\n    try:\n        # Download and save the image\n        with open(image_path, 'wb') as f:\n            f.write(request.urlopen(image_url, timeout=10).read())\n        return True  \n    except Exception:\n        return False  # Failed download\n\n# Function to process a single row in the dataframe\ndef process_row(index, row):\n    if row[\"hasImage\"] and row[\"image_url\"] not in [\"\", \"nan\"]:\n        success = download_image(row)\n        if not success:\n            return index  # Mark row for removal if download fails\n    return None\n\n# Use ThreadPoolExecutor to download images concurrently\nfailed_indices = []\nwith ThreadPoolExecutor(max_workers=10) as executor:  \n    futures = {executor.submit(process_row, index, row): index for index, row in df.iterrows()}\n    \n    for future in as_completed(futures):\n        try:\n            failed_index = future.result()\n            if failed_index is not None:\n                failed_indices.append(failed_index)\n        except Exception as exc:\n            print(f\"An error occurred for row {futures[future]}: {exc}\")\n\n# Remove failed rows from the dataframe\ndf.drop(index=failed_indices, inplace=True)\ndf.reset_index(drop=True, inplace=True)\n\nprint(f\"Downloaded all images. Skipped {len(failed_indices)} failed downloads.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:31:58.856749Z","iopub.execute_input":"2025-03-01T13:31:58.857066Z","iopub.status.idle":"2025-03-01T13:35:39.395655Z","shell.execute_reply.started":"2025-03-01T13:31:58.857038Z","shell.execute_reply":"2025-03-01T13:35:39.394626Z"}},"outputs":[{"name":"stdout","text":"Downloaded all images. Skipped 5550 failed downloads.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# # Plotting images to test download\n# for i in range(5):\n#     path = \"/kaggle/working/images/\" + df[\"id\"][i] + \".jpg\"\n\n#     im= np.array(Image.open(path))\n\n#     print(im.shape)\n#     ax= plt.subplot(121)\n#     ax.imshow(im)\n\n#     plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:35:39.396740Z","iopub.execute_input":"2025-03-01T13:35:39.397012Z","iopub.status.idle":"2025-03-01T13:35:39.401073Z","shell.execute_reply.started":"2025-03-01T13:35:39.396991Z","shell.execute_reply":"2025-03-01T13:35:39.400087Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# from PIL import Image\n\n# # Load the RGBA image\n# image_path = \"/kaggle/working/images/\" + df[\"id\"][0] + \".jpg\"\n# image = Image.open(image_path).convert(\"RGB\")\n\n# # Split the image into individual channels\n# r, g, b = image.split()\n\n# # Plot each channel separately\n# plt.figure(figsize=(10, 5))\n\n# plt.subplot(1, 4, 1)\n# plt.imshow(r)\n# plt.title('Red Channel')\n\n# plt.subplot(1, 4, 2)\n# plt.imshow(g)\n# plt.title('Green Channel')\n\n# plt.subplot(1, 4, 3)\n# plt.imshow(b)\n# plt.title('Blue Channel')\n\n# #plt.subplot(1, 4, 4)\n# #plt.imshow(a)\n# #plt.title('Alpha Channel')\n\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:35:39.402291Z","iopub.execute_input":"2025-03-01T13:35:39.402637Z","iopub.status.idle":"2025-03-01T13:35:39.418551Z","shell.execute_reply.started":"2025-03-01T13:35:39.402587Z","shell.execute_reply":"2025-03-01T13:35:39.417680Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**There are images of different sizes which will not work for CNN:**\n- Check for corrupt image files to avoid errors\n- Resize images to (256, 256, 3)\n- Test ","metadata":{}},{"cell_type":"markdown","source":"> While attempting to resize images, there are errors with some corrupt image files (presumably downloaded from now-defunct links) so I run through image dataset to ensure all images can be successfully opened, else I dropped the corresponding rows from the Dataframe.","metadata":{}},{"cell_type":"code","source":"def validate_images(directory):\n    corrupted_files = []\n\n    # Walk through directory and sub-directories\n    for index, row in df.iterrows():\n      image_path = \"/kaggle/working/images/\" + row[\"id\"] + \".jpg\"\n      try:\n          with Image.open(image_path) as img:\n              img.verify()\n      except Exception as e:\n          corrupted_files.append(image_path)\n          # print(f\"Error with {image_path}: {e}\")\n          df.drop(index=index, axis=0, inplace=True)\n\n    return corrupted_files\n\n\n# Example usage:\ndirectory = \"/kaggle/working/images/\"\ncorrupted_images = validate_images(directory)\nif corrupted_images:\n    print(f\"Found {len(corrupted_images)} corrupted images.\")\nelse:\n    print(\"All images are valid!\")\ndf.reset_index(drop=True, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:35:39.419483Z","iopub.execute_input":"2025-03-01T13:35:39.419696Z","iopub.status.idle":"2025-03-01T13:35:41.350784Z","shell.execute_reply.started":"2025-03-01T13:35:39.419679Z","shell.execute_reply":"2025-03-01T13:35:41.349906Z"}},"outputs":[{"name":"stdout","text":"Found 47 corrupted images.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Resized all the images in dataset to a standard size of (256, 256) using PyTorch's `torchvision.transforms`. The resized images overwrite the original ones.\n- Each image is converted to RGB format to ensure consistent color channels.\n- Used PyTorch's `torchvision.transforms.v2.Resize` to resize the image to the specified new_size.\n- This is so I can pass all the images as uniform (256, 256, 3) matrixes in the CNN.","metadata":{}},{"cell_type":"code","source":"# Resize all images to (256, 256, 3)\nnew_size = (256, 256)\n\nfor index, row in df.iterrows():\n    image_path = \"/kaggle/working/images/\" + row[\"id\"] + \".jpg\"\n    image = Image.open(image_path).convert(\"RGB\")\n    \n    resize_transform = v2.Resize(new_size)\n    resized_image = resize_transform(image)\n    resized_image.save(image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:35:41.352137Z","iopub.execute_input":"2025-03-01T13:35:41.352488Z","iopub.status.idle":"2025-03-01T13:36:10.323431Z","shell.execute_reply.started":"2025-03-01T13:35:41.352449Z","shell.execute_reply":"2025-03-01T13:36:10.322268Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# # Plotting images to test resize\n# for i in range(5):\n#     path = \"/kaggle/working/images/\" + df[\"id\"][i] + \".jpg\"\n\n#     im = np.array(Image.open(path))\n\n#     print(im.shape)\n#     ax = plt.subplot(121)\n#     ax.imshow(im)\n\n#     plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:10.324575Z","iopub.execute_input":"2025-03-01T13:36:10.324821Z","iopub.status.idle":"2025-03-01T13:36:10.328955Z","shell.execute_reply.started":"2025-03-01T13:36:10.324800Z","shell.execute_reply":"2025-03-01T13:36:10.327993Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### **Define Bert tokenizer to convert \"clean_title\" strings to uniform 768-element arrays**","metadata":{}},{"cell_type":"markdown","source":"- Use pre-trained BERT model from the Hugging Face `transformers` library to generate embeddings.\n- BERT-Large Uncased model has a feed-forward network of 1024 hidden dimensions which results in a uniform length array.","metadata":{}},{"cell_type":"code","source":"!pip install --quiet transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:10.329764Z","iopub.execute_input":"2025-03-01T13:36:10.330066Z","iopub.status.idle":"2025-03-01T13:36:19.885157Z","shell.execute_reply.started":"2025-03-01T13:36:10.330046Z","shell.execute_reply":"2025-03-01T13:36:19.884129Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import BertModel, BertTokenizer\n\n\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nbert_model = BertModel.from_pretrained(model_name, output_hidden_states=True)\n\n# Put the model in evaluation mode, which turns off dropout regularization which is used in training.\nbert_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:19.886671Z","iopub.execute_input":"2025-03-01T13:36:19.887052Z","iopub.status.idle":"2025-03-01T13:36:24.473215Z","shell.execute_reply.started":"2025-03-01T13:36:19.887016Z","shell.execute_reply":"2025-03-01T13:36:24.472301Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0ea6c325cd45d4afe66a891d2a82e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c35d545c898743d5b2f3fcfcde91eca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65dd9db7b52a4a708c4bb2d0bc6c80a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92f3aedf811459ba6bd21235f4d9897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c27df45378a4753807e63e0828e5c0b"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"def get_bert_embedding(text):\n    inputs = tokenizer.encode_plus(\n        text, add_special_tokens=True,\n        return_tensors=\"pt\",\n        max_length=80,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    \n    return inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0)\n\n\n# Test embedding\ntext = \"this is an embedding example for fake news detection\"\ninput_ids, attention_mask = get_bert_embedding(text=text)\nprint(input_ids.shape)\nprint(attention_mask.shape)\nprint(input_ids)\nprint(attention_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:24.474446Z","iopub.execute_input":"2025-03-01T13:36:24.475050Z","iopub.status.idle":"2025-03-01T13:36:24.486728Z","shell.execute_reply.started":"2025-03-01T13:36:24.475001Z","shell.execute_reply":"2025-03-01T13:36:24.485906Z"}},"outputs":[{"name":"stdout","text":"torch.Size([80])\ntorch.Size([80])\ntensor([  101,  2023,  2003,  2019,  7861,  8270,  4667,  2742,  2005,  8275,\n         2739, 10788,   102,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### **Load and process input data**","metadata":{}},{"cell_type":"code","source":"df_train, df_test = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df[\"2_way_label\"]\n)\n\ndf_test, df_val = train_test_split(\n    df_test,\n    test_size=0.5,\n    stratify=df_test[\"2_way_label\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:24.487777Z","iopub.execute_input":"2025-03-01T13:36:24.488065Z","iopub.status.idle":"2025-03-01T13:36:24.549392Z","shell.execute_reply.started":"2025-03-01T13:36:24.488042Z","shell.execute_reply":"2025-03-01T13:36:24.548741Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"Create FakedditDataset class:\n- Apply transforms to images\n- Returns:\n    - `Image` in form of tensor\n    - BERT tokenizer `input_ids` and `attention_mask`\n    - Corresponding `6_way_label`","metadata":{}},{"cell_type":"code","source":"# class FakedditDataset(Dataset):\n#     def __init__(self, df, text_field=\"clean_title\", label_field=\"6_way_label\", image_id=\"id\"):\n#         self.df = df.reset_index(drop=True)\n#         self.text_field = text_field\n#         self.label_field = label_field\n#         self.image_id = image_id\n        \n#         self.image_size = (256, 256)\n#         # Using the pre-calculated ImageNet mean and std values for normalization\n#         self.mean, self.std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n        \n#         self.transform_func = v2.Compose([\n#             v2.Resize(self.image_size),\n#             v2.ToImage(),\n#             v2.ToDtype(torch.float32, scale=True),\n# #             v2.Normalize(self.mean, self.std)\n#         ])\n        \n#     def __getitem__(self, index):\n#         text = str(self.df.at[index, self.text_field])\n#         label = self.df.at[index, self.label_field]\n        \n#         image_path = \"/kaggle/working/images/\" + self.df.at[index, self.image_id] + \".jpg\"\n#         image = Image.open(image_path)\n#         image = self.transform_func(image)\n        \n#         input_ids, attention_mask = get_bert_embedding(text)\n        \n#         return image, input_ids, attention_mask, label\n        \n        \n#     def __len__(self):\n#         return self.df.shape[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:24.550428Z","iopub.execute_input":"2025-03-01T13:36:24.550788Z","iopub.status.idle":"2025-03-01T13:36:24.555145Z","shell.execute_reply.started":"2025-03-01T13:36:24.550753Z","shell.execute_reply":"2025-03-01T13:36:24.554224Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class FakedditDataset(Dataset):\n    def __init__(self, df, text_field=\"clean_title\", label_field=\"2_way_label\", \n                 image_id=\"id\", is_training=True):\n        self.df = df.reset_index(drop=True)\n        self.text_field = text_field\n        self.label_field = label_field\n        self.image_id = image_id\n        self.is_training = is_training\n        \n        # Training augmentations\n        if is_training:\n            self.transform_func = v2.Compose([\n                v2.Resize((256, 256)),            # Resize to a slightly larger size for random cropping\n                v2.RandomCrop((224, 224)),          # Crop to the expected 224x224 resolution for SigLIP\n                v2.RandomHorizontalFlip(),          # Data augmentation: horizontal flip\n                v2.ColorJitter(brightness=0.2, contrast=0.2),  # Random color adjustments\n                v2.ToImage(),                     # Ensure image format\n                v2.ToDtype(torch.float32, scale=True),  # Convert to float and scale pixel values\n                v2.Normalize([0.485, 0.456, 0.406],  # Normalize using ImageNet stats (adjust if necessary)\n                             [0.229, 0.224, 0.225])\n            ])\n        else:\n            self.transform_func = v2.Compose([\n                v2.Resize((224, 224)),            # Directly resize to 224x224 for validation/test\n                v2.ToImage(),\n                v2.ToDtype(torch.float32, scale=True),\n                v2.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n            ])\n            \n            \n    def __getitem__(self, index):\n        text = str(self.df.at[index, self.text_field])\n        label = self.df.at[index, self.label_field]\n        \n        image_path = \"/kaggle/working/images/\" + self.df.at[index, self.image_id] + \".jpg\"\n        image = Image.open(image_path)\n        image = self.transform_func(image)\n        \n        input_ids, attention_mask = get_bert_embedding(text)\n        \n        return image, input_ids, attention_mask, label, index\n        \n        \n    def __len__(self):\n        return self.df.shape[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:24.556227Z","iopub.execute_input":"2025-03-01T13:36:24.556482Z","iopub.status.idle":"2025-03-01T13:36:24.575440Z","shell.execute_reply.started":"2025-03-01T13:36:24.556452Z","shell.execute_reply":"2025-03-01T13:36:24.574607Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"train_data = FakedditDataset(df_train)\nval_data = FakedditDataset(df_val, is_training=False)\ntest_data = FakedditDataset(df_test, is_training=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:24.580499Z","iopub.execute_input":"2025-03-01T13:36:24.580784Z","iopub.status.idle":"2025-03-01T13:36:24.597829Z","shell.execute_reply.started":"2025-03-01T13:36:24.580767Z","shell.execute_reply":"2025-03-01T13:36:24.597170Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=16, shuffle=True)\n\n# Verify if dataset is created accurately\nimage, input_ids, attention_mask, label, indices = next(iter(train_loader))\nprint(input_ids.shape, attention_mask.shape, label.shape, image.shape, indices.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:24.598722Z","iopub.execute_input":"2025-03-01T13:36:24.598997Z","iopub.status.idle":"2025-03-01T13:36:24.707490Z","shell.execute_reply.started":"2025-03-01T13:36:24.598976Z","shell.execute_reply":"2025-03-01T13:36:24.706567Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 80]) torch.Size([16, 80]) torch.Size([16]) torch.Size([16, 3, 224, 224]) torch.Size([16])\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### Training and evaluate the model","metadata":{}},{"cell_type":"markdown","source":"- **Loss function:** `Weighted Cross Entropy Loss` with softmax activation function to return final probability.\n    - Apply the `compute_class_weight` to calculate percentage values per class for a weighted CrossEntropy. Since some classes have considerably more samples than others, all classes are weighted and taken as input into the loss calculation according to their respective number of samples\n- **Early stopping:** Stop training if loss is increasing.\n- **Optimizer:** Adam\n- **Learning rate:** 1e-4\n- **Epochs:** 10","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=4, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.delta = delta\n\n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:24.708627Z","iopub.execute_input":"2025-03-01T13:36:24.708945Z","iopub.status.idle":"2025-03-01T13:36:24.715413Z","shell.execute_reply.started":"2025-03-01T13:36:24.708922Z","shell.execute_reply":"2025-03-01T13:36:24.714381Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n    early_stopping = EarlyStopping(patience=5, verbose=True)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n\n        for img, input_ids, attention_mask, label, indices in train_loader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            label = label.to(device).float()  # Ensure label is float for BCE\n            img = img.to(device)\n                \n            optimizer.zero_grad()\n\n            # Forward pass and squeeze the extra dimension\n            outputs = model(img, input_ids, attention_mask)\n            outputs = outputs.squeeze(1)\n            loss = criterion(outputs, label)\n\n            # Backward pass and optimization\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            running_loss += loss.item() * img.size(0)\n            \n        # Validation loop\n        model.eval()\n        val_loss = 0.0\n        correct_preds = 0\n        with torch.no_grad():\n            for img, input_ids, attention_mask, label, indices in val_loader:\n                input_ids = input_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                label = label.to(device).float()\n                img = img.to(device)\n    \n                outputs = model(img, input_ids, attention_mask)\n                outputs = outputs.squeeze(1)\n                loss = criterion(outputs, label)\n                val_loss += loss.item() * img.size(0)\n\n                # Use sigmoid and threshold at 0.5 for binary prediction\n                preds = (torch.sigmoid(outputs) > 0.5).float()\n                correct_preds += torch.sum(preds == label)\n\n        val_loss = val_loss / len(val_loader.dataset)\n        accuracy = correct_preds.double() / len(val_loader.dataset)\n        scheduler.step(val_loss)\n        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader.dataset):.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n\n        early_stopping(val_loss)\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered. Stopping training.\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:54:34.078405Z","iopub.execute_input":"2025-03-01T13:54:34.079082Z","iopub.status.idle":"2025-03-01T13:54:34.088169Z","shell.execute_reply.started":"2025-03-01T13:54:34.079051Z","shell.execute_reply":"2025-03-01T13:54:34.087277Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\ndef evaluate_model(model, test_loader, criterion):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            images = batch['image'].to(device)\n            text_input_ids = batch['text_input_ids'].to(device)\n            text_attention_mask = batch['text_attention_mask'].to(device)\n            labels = batch['label'].to(device).float()\n\n            outputs = model(images, text_input_ids, text_attention_mask)\n            outputs = outputs.squeeze(1)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n    print(\"Test Loss:\", total_loss/len(test_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:54:36.208066Z","iopub.execute_input":"2025-03-01T13:54:36.209146Z","iopub.status.idle":"2025-03-01T13:54:36.214594Z","shell.execute_reply.started":"2025-03-01T13:54:36.209114Z","shell.execute_reply":"2025-03-01T13:54:36.213800Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"### **Build multimodal BERT + ResNet50**","metadata":{}},{"cell_type":"code","source":"class BERTSigLIPClassifier(nn.Module):\n    def __init__(self, num_classes=2, dropout_rate=0.3):\n        super(BERTSigLIPClassifier, self).__init__()\n        \n        # Image processing with SigLIP\n        # Import SigLIP model from torchvision or transformers\n        from transformers import AutoProcessor, AutoModel\n        \n        # SigLIP model\n        self.image_processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n        self.image_model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n        \n        # Freeze early layers of SigLIP\n        for i, (name, param) in enumerate(self.image_model.vision_model.named_parameters()):\n            # Freeze all except the final transformer blocks\n            if \"encoder.layers\" in name:\n                layer_num = int(name.split(\"encoder.layers.\")[1].split(\".\")[0])\n                if layer_num < 10:  # Freeze first 10 layers (adjust as needed)\n                    param.requires_grad = False\n            elif \"layernorm\" not in name and \"pooler\" not in name:\n                param.requires_grad = False\n        \n        # SigLIP vision embedding dimension is typically 768\n        siglip_embedding_dim = self.image_model.vision_model.config.hidden_size\n                    \n        # Modify image branch with more regularization\n        self.fc_image = nn.Sequential(\n            nn.Linear(siglip_embedding_dim, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(dropout_rate)\n        )\n        \n        # Text processing\n        self.text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n        # Freeze BERT layers except last few\n        for param in self.text_model.parameters():\n            param.requires_grad = False\n        for param in self.text_model.encoder.layer[-2:].parameters():\n            param.requires_grad = True\n            \n        # Modify text branch with more regularization\n        self.fc_text = nn.Sequential(\n            nn.Linear(self.text_model.config.hidden_size, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(dropout_rate)\n        )\n        \n        # Fusion and classification layers\n        self.classifier = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(dropout_rate),\n            nn.Linear(256, 1)\n        )\n        \n    def forward(self, image, text_input_ids, text_attention_mask):\n        # Image branch\n        # SigLIP expects raw pixel values and handles preprocessing internally\n        vision_outputs = self.image_model.vision_model(\n            pixel_values=image,\n            return_dict=True\n        )\n        x_img = vision_outputs.pooler_output  # Get the pooled image representation\n        x_img = self.fc_image(x_img)\n        \n        # Text branch with attention-weighted pooling\n        text_outputs = self.text_model(\n            input_ids=text_input_ids,\n            attention_mask=text_attention_mask,\n            return_dict=True\n        )\n        \n        # Attention-weighted pooling\n        attention_weights = text_attention_mask.unsqueeze(-1).float()\n        x_text = torch.sum(text_outputs.last_hidden_state * attention_weights, dim=1)\n        x_text = x_text / torch.sum(attention_weights, dim=1)\n        x_text = self.fc_text(x_text)\n        \n        # Maximum fusion (keeping the same fusion strategy you had)\n        x_fused = torch.max(x_text, x_img)\n        \n        # Classification\n        x_out = self.classifier(x_fused)\n        return x_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:54:39.485786Z","iopub.execute_input":"2025-03-01T13:54:39.486132Z","iopub.status.idle":"2025-03-01T13:54:39.496944Z","shell.execute_reply.started":"2025-03-01T13:54:39.486107Z","shell.execute_reply":"2025-03-01T13:54:39.496049Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\n\n# Compute class weights from training labels\nlabels = df_train['2_way_label'].to_numpy()\n# Compute weights for each class (0 and 1)\nclass_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n# For BCEWithLogitsLoss, we use pos_weight for the positive class (class 1)\n# pos_weight is expected to be a tensor of size [1]\npos_weight = torch.tensor([class_weights[1]], dtype=torch.float).to(device)\n\n# Initialize your model (note: num_classes is set to 1 for binary classification)\nmodel = BERTSigLIPClassifier(num_classes=1)\nmodel = model.to(device)\n\n# Define criterion using pos_weight\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=1e-5, factor=0.5, patience=1, verbose=True)\nnum_epochs = 10\ntrain_model(model, train_loader,val_loader, criterion, optimizer, scheduler, num_epochs)\n#print(\"\\n\")\nevaluate_model(model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:54:42.025039Z","iopub.execute_input":"2025-03-01T13:54:42.025676Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Training Loss: 0.5293, Validation Loss: 0.4946, Accuracy: 0.7661\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:52:56.624464Z","iopub.status.idle":"2025-03-01T13:52:56.624723Z","shell.execute_reply.started":"2025-03-01T13:52:56.624597Z","shell.execute_reply":"2025-03-01T13:52:56.624608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Visualize prediction after training**","metadata":{}},{"cell_type":"code","source":"# Assuming you already have a test/validation DataLoader\nmodel.eval()  # Set the model to evaluation mode\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for img, input_ids, attention_mask, label, indices in test_loader:\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        label = label.to(device)\n        img = img.to(device)\n\n        outputs = model(\n              image = img,\n              text_input_ids = input_ids,\n              text_attention_mask = attention_mask\n        )\n\n        # Final Softmax layer returns class predictions per sample in batch\n        # Highest probability value resembles class prediction and is assigned to preds variable\n        _, preds = torch.max(outputs, dim=1)\n        #print(outputs)\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(label.cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:49.333654Z","iopub.status.idle":"2025-03-01T13:36:49.334107Z","shell.execute_reply.started":"2025-03-01T13:36:49.333891Z","shell.execute_reply":"2025-03-01T13:36:49.333909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Generate confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nclass_names = [\n    'TRUE', \n    \"FALSE\"\n]\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.xticks(rotation=45)\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:49.335410Z","iopub.status.idle":"2025-03-01T13:36:49.335824Z","shell.execute_reply.started":"2025-03-01T13:36:49.335605Z","shell.execute_reply":"2025-03-01T13:36:49.335623Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> The model performs quite well on the `TRUE` AND `FALSE CONNECTION` classes but struggles with others.","metadata":{}},{"cell_type":"markdown","source":"### **Visualize the percentage of false predictions per class**","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Calculate false predictions for each class\ntotal_per_class = np.sum(cm, axis=1)\ncorrect_per_class = np.diagonal(cm)\nfalse_per_class = total_per_class - correct_per_class\n\n# Calculate percentage of false predictions per class\nfalse_percentage_per_class = (false_per_class / total_per_class) * 100\n\n# Display the results\nfor idx, class_name in enumerate(class_names):\n    print(f\"Class '{class_name}' - False Predictions: {false_percentage_per_class[idx]:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:49.337681Z","iopub.status.idle":"2025-03-01T13:36:49.338026Z","shell.execute_reply.started":"2025-03-01T13:36:49.337876Z","shell.execute_reply":"2025-03-01T13:36:49.337895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> The model's difficulty in classifying MISLEADING CONTENT and IMPOSTER CONTENT could stem from class imbalance","metadata":{}},{"cell_type":"code","source":"total_per_class","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:49.339238Z","iopub.status.idle":"2025-03-01T13:36:49.339530Z","shell.execute_reply.started":"2025-03-01T13:36:49.339393Z","shell.execute_reply":"2025-03-01T13:36:49.339405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find class with most false predictions\nmax_false_class_idx = np.argmax(false_percentage_per_class)\nprint(f\"Class with most false predictions: {class_names[max_false_class_idx]} ({false_percentage_per_class[max_false_class_idx]:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:49.340664Z","iopub.status.idle":"2025-03-01T13:36:49.340998Z","shell.execute_reply.started":"2025-03-01T13:36:49.340822Z","shell.execute_reply":"2025-03-01T13:36:49.340835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot false prediction percentages as a bar chart\nplt.figure(figsize=(10, 6))\nplt.bar(class_names, false_percentage_per_class, width=0.8, color='orange')\nplt.ylabel('Percentage of False Predictions')\nplt.xticks(rotation=45)\nplt.title('False Predictions per Class')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:49.342337Z","iopub.status.idle":"2025-03-01T13:36:49.342634Z","shell.execute_reply.started":"2025-03-01T13:36:49.342494Z","shell.execute_reply":"2025-03-01T13:36:49.342510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Class distribution histogram**","metadata":{}},{"cell_type":"code","source":"plt.hist(df_train[\"6_way_label\"], bins=np.arange(len(class_names)+1) -0.5, rwidth=0.5)\nplt.xticks(range(len(class_names)), class_names, rotation=45)\nplt.title('Class Distribution in Training Set')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:49.343748Z","iopub.status.idle":"2025-03-01T13:36:49.344049Z","shell.execute_reply.started":"2025-03-01T13:36:49.343913Z","shell.execute_reply":"2025-03-01T13:36:49.343926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import textwrap\n\ndef display_predictions(test_loader, model, device):\n    \"\"\"\n    Displays 4 images per class with true labels, predicted labels, and clean titles\n    \"\"\"\n    model.eval()\n    \n    # Initialize figure (6 classes × 4 samples per class)\n    fig, axs = plt.subplots(6, 4, figsize=(30, 36))\n    plt.subplots_adjust(hspace=1.0)  # Increased spacing for title text\n    \n    class_names = [\n        'TRUE', \n        'SATIRE', \n        'FALSE CONNECTION', \n        'IMPOSTER CONTENT', \n        'MANIPULATED CONTENT', \n        'MISLEADING CONTENT'\n    ]\n    \n    # Dictionary to store samples for each class\n    class_samples = {i: {'correct': [], 'incorrect': []} for i in range(6)}\n\n    class_titles = []\n    \n    # Collect samples\n    with torch.no_grad():\n        for images, input_ids, attention_mask, labels, indices in test_loader:\n\n            clean_titles = test_loader.dataset.df.iloc[indices]['clean_title'].tolist()\n            \n            images = images.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            \n            outputs = model(images, input_ids, attention_mask)\n            predictions = torch.argmax(outputs, dim=1)\n            \n            # Store samples with their titles\n            for img, true_label, pred_label, title in zip(images, labels, predictions, clean_titles):\n                true_label = true_label.item()\n                pred_label = pred_label.cpu().item()\n                \n                is_correct = true_label == pred_label\n                category = 'correct' if is_correct else 'incorrect'\n                \n                if len(class_samples[true_label][category]) < 2:\n                    class_samples[true_label][category].append((\n                        img.cpu(),\n                        true_label,\n                        pred_label,\n                        title\n                    ))\n            \n            if all(len(samples['correct']) == 2 and len(samples['incorrect']) == 2 \n                   for samples in class_samples.values()):\n                break\n    \n    # Display images\n    for class_idx in range(6):\n        # Add true label on the right\n        fig.text(0.92, 0.85 - (class_idx * 0.135), \n                f\"True Class:\\n{class_names[class_idx]}\", \n                fontsize=22, \n                verticalalignment='center')\n        \n        # Display correct predictions\n        for i, (img, true_label, pred_label, title) in enumerate(class_samples[class_idx]['correct']):\n            ax = axs[class_idx, i]\n            \n            img_np = np.transpose(img.numpy(), (1, 2, 0))\n            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n            \n            ax.imshow(img_np)\n            \n            # Add predicted label on top\n            ax.set_title(f'Predicted: {class_names[pred_label]}', \n                        {'color': 'green', 'fontsize': 20},\n                        pad=20)\n\n            # Wrap clean_title if it is too long\n            wrapped_title = \"\\n\".join(textwrap.wrap(title, width=30))  # Adjust width as needed\n            \n            # Add clean_title at the bottom\n            ax.text(0.5, -0.3, f\"Title: {wrapped_title}\", \n                   horizontalalignment='center',\n                   verticalalignment='center',\n                   transform=ax.transAxes,\n                   fontsize=20,\n                   wrap=True)\n            \n            ax.axis('off')\n        \n        # Display incorrect predictions\n        for i, (img, true_label, pred_label, title) in enumerate(class_samples[class_idx]['incorrect']):\n            ax = axs[class_idx, i+2]\n            \n            img_np = np.transpose(img.numpy(), (1, 2, 0))\n            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n            \n            ax.imshow(img_np)\n            \n            # Add predicted label on top\n            ax.set_title(f'Predicted: {class_names[pred_label]}', \n                        {'color': 'red', 'fontsize': 20},\n                        pad=20)\n\n            # Wrap clean_title if it is too long\n            wrapped_title = \"\\n\".join(textwrap.wrap(title, width=30))  # Adjust width as needed\n            \n            # Add clean_title at the bottom\n            ax.text(0.5, -0.3, f\"Title: {wrapped_title}\", \n                   horizontalalignment='center',\n                   verticalalignment='center',\n                   transform=ax.transAxes,\n                   fontsize=20,\n                   wrap=True)\n            \n            ax.axis('off')\n    \n    # Add column labels\n    fig.text(0.35, 0.95, 'Correct Predictions', fontsize=20, ha='center')\n    fig.text(0.7, 0.95, 'Incorrect Predictions', fontsize=20, ha='center')\n    \n    plt.show()\n\n\n# Usage\nmodel.to(device)\ndisplay_predictions(test_loader, model, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T13:36:49.345432Z","iopub.status.idle":"2025-03-01T13:36:49.345726Z","shell.execute_reply.started":"2025-03-01T13:36:49.345592Z","shell.execute_reply":"2025-03-01T13:36:49.345603Z"}},"outputs":[],"execution_count":null}]}